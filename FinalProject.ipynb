{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Varij-Saini/Final-Project-ML/blob/main/FinalProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_LMHfc5af0b"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded=files.upload()"
      ],
      "metadata": {
        "id": "fqSWT2iQm-uw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "dataset = pd.read_csv(io.BytesIO(uploaded['stroke-data.csv']))\n",
        "dataset.drop('id', inplace=True, axis=1)"
      ],
      "metadata": {
        "id": "qxvgjZYjobw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "id": "2MSn5DS9dS5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fill missing values of BMI with mean \n",
        "bmi_mean=dataset['bmi'].mean()\n",
        "dataset['bmi']=dataset['bmi'].fillna(bmi_mean)\n"
      ],
      "metadata": {
        "id": "M_A_wgFypZsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The big picture of the dataset\n",
        "import seaborn as sns\n",
        "sns.pairplot(data=dataset,hue='stroke',palette='bright')"
      ],
      "metadata": {
        "id": "S99iEGoIvfSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "metadata": {
        "id": "3-6d5ytxtHKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import label encoder \n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# label_encoder object knows how to understand word labels \n",
        "label_encoder = LabelEncoder()\n",
        "# Encode labels in columns\n",
        "dataset['gender']= label_encoder.fit_transform(dataset['gender'])\n",
        "dataset['ever_married']= label_encoder.fit_transform(dataset['ever_married']) \n",
        "dataset['work_type']= label_encoder.fit_transform(dataset['work_type']) \n",
        "dataset['Residence_type']= label_encoder.fit_transform(dataset['Residence_type'])\n",
        "dataset['smoking_status']= label_encoder.fit_transform(dataset['smoking_status'])   \n",
        "print(dataset.head())"
      ],
      "metadata": {
        "id": "ExRkQU0O0cQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Separate dataset into features and target value\n",
        "X = dataset.drop(['stroke'],axis=1)\n",
        "Y = dataset.iloc[:, -1]"
      ],
      "metadata": {
        "id": "0kj6sSZvtXik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data for training and testing \n",
        "X_train, X_test, Y_train , Y_test = train_test_split(X,Y, test_size=0.2 , random_state=42)"
      ],
      "metadata": {
        "id": "sqp-6m17uCSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "# Scale the training Data\n",
        "SC=MinMaxScaler()\n",
        "X_train = SC.fit_transform(X_train)"
      ],
      "metadata": {
        "id": "cm-rT1mMzL_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Total count of people not having stroke and people having stroke along with its graph\n",
        "print(dataset['stroke'].value_counts())\n",
        "dataset['stroke'].value_counts().sort_index().plot.bar()"
      ],
      "metadata": {
        "id": "2XZkXlWW3kba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see from the above graph that the data is imbalanced\n"
      ],
      "metadata": {
        "id": "SC2O8LSZaRBq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep Neural network"
      ],
      "metadata": {
        "id": "xdlOQDhhfGi2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from keras.layers import Dense\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#training and validation set \n",
        "x1_train, x1_test, y1_train, y1_test = train_test_split(X,Y, test_size=0.5, random_state=42)\n",
        "x1_train = SC.fit_transform(x1_train)\n",
        "\n",
        "#create DNN model and compile with optimizer\n",
        "model = keras.models.Sequential()\n",
        "model.add(Dense(15, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='Adam',\n",
        "              loss= keras.losses.MeanSquaredError(),\n",
        "              metrics=['mae'])\n",
        "#fit the model \n",
        "history = model.fit(x1_train, y1_train, epochs=100, batch_size=35, validation_data=(x1_test, y1_test))\n",
        "\n",
        "#get each loss value to plot on graph\n",
        "loss = history.history['loss']\n",
        "mae = history.history['mae']\n",
        "val_loss = history.history['val_loss']\n",
        "val_mae = history.history['val_mae']\n",
        "\n",
        "#plot all values over 100 epochs\n",
        "epochs = range(1,101)\n",
        "plt.plot(epochs, loss, 'g', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='validation loss')\n",
        "plt.plot(epochs, mae, 'r', label='mae')\n",
        "plt.plot(epochs, val_mae, 'y', label='val_mae')\n",
        "plt.title('Graph of Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KfrOvlbU4ioh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizations of DNN "
      ],
      "metadata": {
        "id": "jQ6kZhfOy_Z0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils.vis_utils import plot_model\n",
        "import visualkeras\n",
        "from ann_visualizer.visualize import ann_viz \n",
        "\n",
        "# 3 different visualizations of the DNN \n",
        "ann_viz(model, title=\"DNN visualiztion\", view=True, filename='DNN')\n",
        "plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True, show_layer_activations=True)\n",
        "visualkeras.layered_view(model, legend=True)\n"
      ],
      "metadata": {
        "id": "s2iOqGtyy-20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Polynomial Classification "
      ],
      "metadata": {
        "id": "SGCvFOMzlrga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "#Scale data\n",
        "sc = StandardScaler()\n",
        "x_train_poly = sc.fit_transform(X_train)\n",
        "\n",
        "#set the SVC with the polynomial kernel function and fit the data\n",
        "classifier = SVC(kernel='poly', random_state= 0)\n",
        "classifier.fit(x_train_poly,Y_train)\n",
        "\n",
        "#check accuracy score based on predicted vs actual values\n",
        "y_pred = classifier.predict(X_test)\n",
        "accuracy_score(Y_test,y_pred)\n"
      ],
      "metadata": {
        "id": "PAFtObPGNN9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest Model(Random forest Classifier)"
      ],
      "metadata": {
        "id": "JeW30UBSdVLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.over_sampling import SMOTE \n",
        "oversample = RandomOverSampler(sampling_strategy='minority') #As we saw above the data was imbalanced, so to balance it, we are using oversampling\n",
        "X=dataset.drop(['stroke'],axis=1)\n",
        "Y=dataset['stroke']\n",
        "X_oversample, y_oversample = oversample.fit_resample(X, Y)\n",
        "oversample=SMOTE() #SMOTE is Synthetic Minority Oversampling technique(we are oversampling the minority that is Stroke(value=1))\n",
        "X_train,Y_train=oversample.fit_resample(X,Y.ravel()) #converting y to 1D array"
      ],
      "metadata": {
        "id": "BiqOwfLbaWWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "random_forest = RandomForestClassifier(n_estimators = 100, criterion= 'entropy', random_state = 0)\n",
        "random_forest.fit(X_train,Y_train)"
      ],
      "metadata": {
        "id": "j4MA_S-uWDo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Accuracy Score\n",
        "from sklearn.metrics import accuracy_score\n",
        "Y_train_rf = random_forest.predict(X_train)\n",
        "accuracy_train_rf = accuracy_score(Y_train, Y_train_rf)\n",
        "\n",
        "y_pred_test_rf = random_forest.predict(X_test)\n",
        "accuracy_test_rf = accuracy_score(Y_test, y_pred_test_rf)\n",
        "print(accuracy_train_rf)\n",
        "print(accuracy_test_rf)\n",
        "\n"
      ],
      "metadata": {
        "id": "WtHL_LAwbgYA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}